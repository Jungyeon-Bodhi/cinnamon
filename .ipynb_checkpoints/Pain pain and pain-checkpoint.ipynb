{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046128aa-9237-47b7-aa05-e9ee8e0f09f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "[1] Set-up Google AI\n",
    "\"\"\"\n",
    "def setup_AI(api):\n",
    "    GOOGLE_API_KEY = api\n",
    "    \n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    \n",
    "    # Set up the model\n",
    "    generation_config = {\n",
    "      \"temperature\": 0.9,\n",
    "      \"top_p\": 1,\n",
    "      \"top_k\": 1,\n",
    "      \"max_output_tokens\": 2100,\n",
    "    }\n",
    "    \n",
    "    model = genai.GenerativeModel('gemini-2.0-flash',generation_config=generation_config)\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "[2] Extracting ASEAN policy documents\n",
    "\"\"\"\n",
    "def read_docs(file_path):\n",
    "    data_folder = file_path  \n",
    "    pdf_files = [f for f in os.listdir(data_folder) if f.endswith(\".pdf\")]\n",
    "    pdf_files = sorted(pdf_files, key=lambda x: int(os.path.splitext(x)[0]))\n",
    "    pdf_numbers = sorted([int(os.path.splitext(f)[0]) for f in pdf_files])\n",
    "    num_pdfs = len(pdf_files)\n",
    "    print(f\"A total of {num_pdfs} PDF files exist.\")\n",
    "    docs = {}\n",
    "    for i, pdf_file in enumerate(pdf_files, start=1):\n",
    "        pdf_path = os.path.join(data_folder, pdf_file)\n",
    "        \n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "            docs[f\"doc{i}\"] = text\n",
    "    return docs, pdf_numbers\n",
    "\n",
    "\"\"\"\n",
    "[3] Generating responses\n",
    "\"\"\"\n",
    "def generate_ai(model, pdf_numbers, docs, file_name):\n",
    "    results = []\n",
    "    for doc, i in zip(list(docs.keys()), pdf_numbers):\n",
    "        prompt = f\"\"\"\n",
    "        {docs[doc]} <- This is the policy document, please answer the following six questions with the score and the reason behind (just a bit strict, transformation and potential for transformation are different).\n",
    "1) Question 1: To what extent does the policy/project address the intended challenge/issue? - Question 1 Criteria is as followed = Score 1 (Poorly) - Does not state the challenge it intends to address/unclear what challenge it intends to address (towards a increased understanding, tolerance and a sense of regional agenda's), Score 2 (Somewhat) - States the challenge it aims to address but not why addressing this matters? (towards a increased understanding, tolerance and a sense of regional agenda's), Score 3 (Greatly) - States what the challenge is clearly and why this challenge is important to fostering a sense of increased understanding, tolerance and a sense of regional agenda's.\n",
    "2) Question 2: To what extent is the policy/project contextually appropriate? - Question 2 Criteria is as followed = Score 1 (poorly) - The project appears at odds with the contextual dynamics of the time it was set up/ The project makes no adaptations to its contextual dynamics. Score 2 (Somewhat) - The project appears to have made some adaptations towards context but could do with improvement/ context has changed since project inception and it has not adapted to new context. Score 3 (greatly) - The project is contextually relevant.\n",
    "3) Question 3: To what extent was the policy/project designed through a consultative process and include meaningful feedback opportunities? - Question 3 Criteria is as followed = Score 1 (poorly) The policy/project faces significant opposition from most or all stakeholders. Or there is a significant opposition to it. Score 2 (somewhat) - The policy/project is mostly supported by key stakeholders with minor opposition. Score 3 (Greatly) - The policy/project enjoys broad support from all or nearly all key stakeholder groups.\n",
    "4) Question 4: To what extent Is the policy/project widely accepted among key stakeholder groups? - Question 4 Criteria is as followed = Score 1 (Poorly) - No signs of acceptance/ outright denied acceptance. Score 2 (somewhat) - Signs of some acceptance. Score 3 (greatly) - Sign of total or majority acceptance.\n",
    "5) Question 5: Is there an operational Monitoring and Evaluation (M&E) framework in place, and to what extent is it adhered to? - Question 5 Criteria is as followed = Score 1 (poorly) - No M&E framework to be found. Score 2 (somewhat) - Appears to be some attempt or mention of  M&E. Score 3 (greatly) - M&E framework intact.\n",
    "6) Question 6: To what extent does the policy/project have transformative potential? - Question 6 Criteria is as followed = Score 1 (poorly) The policy/project introduces minor or incremental changes with no significant disruption to existing systems/It lacks scalability and is unlikely to inspire further innovation/ Short-term focus and impacts. Score 2 (somewhat) - The policy/project brings noticeable improvements but does not fundamentally alter existing structures/ It has some scalability, but implementation may face barriers. Score 3 (greatly) - The policy/project introduces groundbreaking or system-wide changes, reshaping existing frameworks/ leads to long-term attitudinal, behavioural or policy changes.\n",
    "\n",
    "\"\"\"    \n",
    "        response = model.generate_content(prompt)\n",
    "        temp = []\n",
    "        results.append({'Document': f'{i}.pdf','Response': response.text})\n",
    "        temp.append({'Document': f'{i}.pdf','Response': response.text})\n",
    "        temp_df = pd.DataFrame(temp)\n",
    "        temp_df.to_excel(f\"data/generated/{i}.xlsx\")\n",
    "        print(f\"Finished: {i}.xlsx\")\n",
    "        time.sleep(90)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(f\"data/{file_name}.xlsx\")\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a226a6-e49b-4421-8558-607cdaf385df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "[4] Pipeline\n",
    "\"\"\"\n",
    "api = 'AIzaSyBwjlLmM1X5brUWVgUkA17GkQuFje8AIXs'\n",
    "file_path = \"data/\"\n",
    "doc_name = \"ASEAN_effectiveness_assessment\"\n",
    "\n",
    "cinnamon = setup_AI(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4d5492-3a28-4ed4-bdc7-02c977f31e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 1 PDF files exist.\n"
     ]
    }
   ],
   "source": [
    "docs, pdf_numbers = read_docs(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7aa30d-a168-4307-8704-82063f50ac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: 25.xlsx\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m generate_ai(cinnamon, pdf_numbers, docs, doc_name)\n",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m, in \u001b[0;36mgenerate_ai\u001b[0;34m(model, pdf_numbers, docs, file_name)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m90\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m results_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "df = generate_ai(cinnamon, pdf_numbers, docs, doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d077619f-1fa4-4d70-b09f-35dcafb6ebfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b38723-b73b-40bb-9fb0-0ac8e6016c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
